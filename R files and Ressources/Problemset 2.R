


## Exercise 1 (25 marks)

# In this exercise you will simulate a version control workflow using git and GitHub. 
# You should create a new public repository, and then use it to demonstrate the pipeline described below. 
# This public repo should not be the same one you use to prepare this assessment. 
# In your .html submission, your answer to this exercise should be a single sentence providing a clickable hyperlink URL 
# for the public repository you create and manipulate 
# (e.g. "My public repository can be found [here](https://github.com/tsrobinson/firstrepository)). 


### Pipeline

#1. A developer creates a new GitHub repository, which contains an R script defining a function with a single argument `data`, that takes a dataset and performs some input transformation on it. This transformation can be as simple or complex as you like, but should work on at least one dataset (either a built-in R dataset, or one provided by you in the public repository). The function should return the transformed data.

#2. The same developer wishes to include new functionality by adding a second argument to *the same function*. Depending on the value passed to that second argument, the function should perform different data transformations. To preserve the integrity of the main branch, they add this functionality in a new branch called "dev", which they do *not* immediately merge into main.

#3. Having implemented this new functionality on the branch "dev", the developer wants to demonstrate to users how it works. They create a new branch called "doc" *off the "dev" branch*, and add an RMarkdown file and knitted .html version that briefly shows what the function outputs depending on the value of this second argument.

# 4. Finally, once the developer is happy with these changes, they first use a pull request to merge the "doc" branch into "dev", and then do the same to merge "dev" into "main".

# *Hints:*
  
# * We will assess you partly by looking at the version control history -- make sure that your commits and pull request descriptions are informative

# * Marks will be available for concise, well-documented code

# * You do not need to worry about the *number* of commits you make: you may make mistakes that need rectifying, or want to make multiple commits to achieve each stage of the above workflow. These actions are fine, so long as you document what each commit does (e.g. "fix issue in function documentation").

# * Before starting this exercise, think carefully about the files you will eventually need to make (and the structure of your repository). Make sure your final repository includes all files mentioned in the workflow and which are needed to run your code. You do not need to submit these separately as part of your submission on Moodle.

"My public repository can be found [here](https://github.com/CharlotteKub/Exercise_1.git)."


## Exercise 2 (30 marks)


library(readr)
library(skimr)
library(texreg)
library(xtable)
library(lubridate)
library(countrycode)

# The OxCGRT study produced time-series data on governments' policy responses to Covid-19, from 2020 to the end of 2022. You can find this data, including a codebook, [here](https://github.com/OxCGRT/covid-policy-dataset) [https://github.com/OxCGRT/covid-policy-dataset/].

# Using the `data/OxCGRT_compact_national_v1.csv` file from this repository, and the **ggplot2** package, you should generate a *single* visualization to help answer the following questions:

# "To what extent did different regions of the world implement some form of recommendation or restriction for citizens to stay at home over the course of 2020-2022? How do the introduction of these restrictions compare to the regions' implementation of income support over the same period?"

# You should present your graphic (remember to introduce it in the text) and, in prose, answer the questions above by pointing to specific features of your visualisation.

# *Notes*: 

# * By "single", we mean something that can be generated by a single call to `plot()` (i.e. multiple facets in the same graphic are acceptable)
# * You should read the documentation on the OxCGRT repository to find the relevant variables
# * You may need to perform some data processing/manipulation/summarisation prior to visualising the data
# * In particular, you may want to simplify the scale of the stay at home directives and income support variables 
  

###### Questions: 
##"To what extent did different regions of the world implement some form of recommendation or restriction for citizens to stay at home over the course of 2020-2022? 
## How do the introduction of these restrictions compare to the regions' implementation of income support over the same period?"
## You should present your graphic (remember to introduce it in the text) and, in prose, answer the questions above by pointing to specific features of your visualisation.


############################
#### Variables I need: #####
############################

# recommendation or restriction for citizens to stay at home: `C6M_Stay at home requirements`
# labels: 
# 0 - no measures
# 1 - recommend not leaving house
# 2 - require not leaving house with exceptions for daily exercise, grocery shopping, and 'essential' trips
# 3 - require not leaving house with minimal exceptions (eg allowed to leave once a week, or only one person can leave at a time, etc)


# income support: _Income support`

# labels: 
# 0 - no income support
# 1 - government is replacing less than 50% of lost salary (or if a flat sum, it is less than 50% median salary)
# 2 - government is replacing 50% or more of lost salary (or if a flat sum, it is greater than 50% median salary)


# time: 

# what about flag variables?? 
# 0 =  "targeted" to a specific geographical region (flag=0) 
# 1 = a "general" policy that is applied across the whole country/territory (flag=1)


OxCGRT <- read_csv("~/Desktop/Data_for_Data_Scientists/OxCGRT_compact_national_v1.csv")
View(OxCGRT)

table(OxCGRT$CountryName)
table(OxCGRT$`E1_Income support`)
skim(OxCGRT$`E1_Income support`)

table(OxCGRT$`C6M_Stay at home requirements`)
table(OxCGRT$C6M_Flag)

table(OxCGRT$RegionName)
table(OxCGRT$Date)

##################################
### Manipulating and Cleaning Data
##################################


# creating smaller dataset

data_plot <- OxCGRT %>% select(c(Date, CountryName, CountryCode, `C6M_Stay at home requirements`, `E1_Income support`))

# converting to proper date variable 
data_plot$Date <- ymd(data_plot$Date)


max(data_plot$Date)
min(data_plot$Date)



# Create a new variable 'Month' representing the month
data_plot$Month <- floor_date(data_plot$Date, unit = "month")
data_plot$Week <- floor_date(data_plot$Date, unit = "week")


# renaming for easier recognition
data_plot <- rename(data_plot, C6M_lockdown_restrictions = `C6M_Stay at home requirements`)
table(data_plot$C6M_lockdown_restrictions)


# creating a continent variable for the different countries to group them 
?countrycode

data_plot$region <- countrycode(data_plot$CountryName, origin = "country.name", destination = "region")
table(data_plot$region)


### simplifying the variables of interest

data_plot <- data_plot %>%
  mutate(lockdown = case_when(C6M_lockdown_restrictions == 0 ~ 0, 
                             C6M_lockdown_restrictions == 1  | C6M_lockdown_restrictions == 2  | C6M_lockdown_restrictions == 3 ~ 1))
                              

data_plot <- data_plot %>%
  mutate(income_support = case_when(`E1_Income support` == 0 ~ 0,
                                    `E1_Income support` == 1 | `E1_Income support` == 2 ~ 1))


data_plot <- data_plot %>%
  mutate(lockdown2 = case_when(C6M_lockdown_restrictions == 0 ~ 'no restrictions', 
                              C6M_lockdown_restrictions == 1  | C6M_lockdown_restrictions == 2  | C6M_lockdown_restrictions == 3 ~ 'some form of restrictions'))


data_plot <- data_plot %>%
  mutate(income_support2 = case_when(`E1_Income support` == 0 ~ 'no income support',
                                    `E1_Income support` == 1 | `E1_Income support` == 2 ~ 'some income support'))



# summary statistics
data_plot %>% group_by(region, Month, lockdown, income_support) %>% tally()

data_plot %>% group_by(region, Month, income_support) %>% tally()

data1 <- data_plot %>%
  group_by(Month, region) %>%
  summarise(share_restrictions = sum(lockdown == 1, na.rm = T)/ sum(!is.na(lockdown)),
            income_support_share = sum(income_support == 1, na.rm = T)/sum(!is.na(income_support))) 

ggplot(data1) + 
  geom_line(aes(x = Month, y = share_restrictions, color = "Covid Restriction")) +
  geom_line(aes(x = Month, y = income_support_share, color = "Income Support")) +
  scale_color_manual(values = c("red", "blue"))  + 
  facet_wrap(~region) +
  labs(x = "Time (in Months)", y = "Share of Countries",
       title = "Regional Differences in Covid Restriction and Income Support (2020-2022)",
       color = "Share of Countries") +
  theme_bw()
  
  
  Gles_Januar_2020 %>%
  mutate(week = floor_date(datetime, "week")) %>% 
  summarise(afd_share = sum(Afd_vote == 1, na.rm = T) / sum(!is.na(Afd_vote)),
            afd_n = sum(Afd_vote, na.rm = T),
            .by = week) 
  
data_plot0 <- data_plot %>%
  group_by(region, Month) %>%
  summarise(Frequency = n()) %>%
  ungroup()


data_plot1 <- data_plot %>%
    group_by(region, Month, lockdown, income_support) %>%
  summarise(Frequency = n()) %>%
  ungroup()

table(data_plot$region)

data_plo2 <- data_plot %>%
  group_by(region) %>%
  summarise(Count = n()) %>%
  ungroup() %>%
  mutate(Relative_Frequency = Count / sum(Count))


data_plot2 <- data_plot %>% group_by(region, Month) %>%
  summarize(avg_lockdown = mean(lockdown), avg_income = mean(income_support))


## basic ggplot: 

ggplot(data_plot1, aes(Month, Frequency, fill = as.factor(region))) + 
  geom_density(stat = "identity", position = "stack", alpha = 0.7) + 
  facet_wrap(~ lockdown) +
  theme_minimal()


ggplot(data_plot1, aes(Month, Frequency, fill = as.factor(region))) + 
  geom_bar(stat = "identity", position = "dodge", alpha = 1) + 
  facet_grid(lockdown ~ income_support) +
  labs(title= "The Difference in Covid Lockdown restrictions and income support across regions from 01.01.2020-31.12.2022",
       fill = "Regions") +
  theme_minimal()

ggplot(data_plot1, aes(Month, Frequency, fill = as.factor(region))) + 
  geom_bar(stat = "identity", position = "stack", alpha = 0.5) + 
  facet_wrap(~lockdown) +
  theme_minimal()

ggplot(data_plot1, aes(Month, Frequency, fill = as.factor(region))) + 
  geom_bar(stat = "identity", position = "stack", alpha = 0.5) + 
  facet_wrap(~income_support) +
  theme_minimal()

ggplot(data_plot2, aes(x = Month, y = avg_lockdown , color = as.factor(region))) +
  geom_line()



?geom_violin
?geom_density
?geom_bar


### Exercise 3 ###

library("utils")
library("quanteda")
library("quanteda.textplots")
library("tidyverse")
library("stringr")

# a. "apple" -> "pple" | "abacus" -> "bacus" | "Annapolis" -> "nnapolis" #first a 
# b. "apple" -> "pple" | "abacus" -> "bcus" | "Annapolis" -> "Annpolis" # all small a
# c. "C1_nat_a" -> "C_a" | "D2_state_g" -> "D_g" | "E_Loc_5_i" -> "E_i" #only first and last character

str_replace(string, pattern, replacement)

str_replace(fruits, "([aeiou])", "")


######################
## Functions in R  ###
######################

function_name <- function(arg1, arg2, ...) {
  # Function body
  # Perform operations and return a result
}


## Function 1: 

no_first_a <- function(x) {
  for (i in x) {
    i <- str_replace(i, "[aA]", "")
    print(i)
  }
}

words1 <- c("apple", "abacus", "Annapolis")

no_first_a("apple")
no_first_a("abacus")
no_first_a("Annapolis")
no_first_a("ananas")

## Function 2: 

no_a <- function(x) {
  for (i in x) {
    i <- str_replace_all(i, "[a]", "")
    print(i)
  }
}

no_a("apple")
no_a("abacus")
no_a("Annapolis")
no_a("ananas")

## Function 3: 

## Text Processing in R
## Character Vectors == Arrays of Strings

str = 'string'
str[1] 

# To get access to the individual characters in an R string, you need to use the substr function:
str = 'string'
substr(str, 1, 1) # =  's'
substr(str, 1, 3) # =  'str'
substr(str, 5, 5) # =  'n'

# nchar = len()

# To break strings apart into vectors of characters, you can use the strsplit function, which works a lot like the split function in Python Here’s an example:

strsplit('0-0-1', '-') # Evaluates to list('0', '0', '1')

# Putting things back together

# You can do this using paste. paste is an idiosyncratic function: it is the only function for concatenation of strings in R, 
# but it also handles the work of more sophisticated functions like Python’s join


str1 = 'first'
str2 = 'second'
print(paste(str1, str))
print(paste(str2, str))

print(paste(str1, str, sep = ''))     # getting rid of the spaces 

# Changing Case

# To change the case of strings or individual characters, you need to use the tolower and toupper functions. 
# You can use these with substr to make a function that turns most common words into their title case form:

To_upper = function(str) {
  substr(str, 1, 1) = toupper(substr(str, 1, 1))
  return(str)
}

To_upper('hello') # = [1] "Hello"


original_string <- "example"
modified_string <- substring(original_string, 1, nchar(original_string) - 1)
print(modified_string)


## first try: 
str = "C1_nat_a"
str1 <- substring(str, 1,1) 
str2 <- substring(str, nchar(str) -1,nchar(str)-0)
str_new <- paste0(str1, str2)
print(str_new)

# creating a function

only_first_and_last <- function(str) {
  str1 <- substr(str,1,1)
  str2 <- substring(str, nchar(str) -1, nchar(str) -0)
  str_new <- paste0(str1, str2)
  print(str_new)
}


only_first_and_last( "C1_nat_a")
only_first_and_last("D2_state_g")
only_first_and_last("E_Loc_5_i")
only_first_and_last("Anana_s")


no_first_letter <- function(str) {
  str = substring(str, 2, nchar(str) -0)
  print(str)
}

no_first_letter("hallole")

### Part 2


# First, develop a **simple** research question that should seek to compare how often a given concept is mentioned across a selection books. 
# An example research question might be: "How often are house pets mentioned in the works of Jane Austen?" 
# Briefly introduce your question and define your concept. Your concept must be measurable using a dictionary of words. 
#For example, if our concept is house pets, we could measure whether they are mentioned by checking for incidences of "dog","cat", and "budgerigar".

# RQ: How does the concept of power vary between 'The Prince' by Machiavelli and 'Leviathan' by Hobbes using words that describe the ruling body and the civil body. 
# choice of books: 'The Prince' by Machiavelli and 'Leviathan' by Hobbes. 

# dictionary: power <- c("ruler", "king", "authority", "government", "sovereignty", "control", "obedience", "social contract", "rights", "self-interest")

# Using the **quanteda** package, build a corpus from your selected books (each document should be the entire book) and a dictionary that encodes your concept. 
# Then, use your dictionary to count the incidences of your concept across these books.


library("utils")
library("quanteda")
library("quanteda.textplots")
library("tidyverse")
library("stringr")


###################################
#### Loading Machiavelli ##########
###################################

prince <- read_file("~/Desktop/machiavelli.txt")


prince_corpus <- corpus(prince, docvars = data.frame(name = "machiavelli"))
print(prince_corpus)
ndoc(prince_corpus)

corp_sent <- corpus_reshape(prince_corpus, to = "sentences")

# subsetting it: 
print(corp_sent[1:162])
prince_corpus_subset <- corpus_subset(corp_sent[163:1501])
prince_corp <- corpus_reshape(prince_corpus_subset, to = "documents")


##### creating dfm

prince_dfm <- prince_corp %>%
  tokens() %>% 
  dfm()

prince_dfm

# When building such a dfm, however, we might e.g. want to avoid counting punctuation or exclude other tokens. 
# This can be done via the tokenisation step:

prince_dfm <- prince_corp %>%
  tokens(remove_punct = TRUE, remove_symbols = TRUE) %>% 
  dfm()
prince_dfm


### creating dictionary

# dictionary: power <- c("ruler", "king", "authority", "government", "sovereignty", "control", "obedience", "social contract", "rights", "self-interest")


power_dictionary <- dictionary(list(ruling_body = c("ruler", "king", "sovereign*", "control","obedience" ),
                                    
                                    civil_body = c("government","social contract", "civil rights"),
                                    power = "power")
)

# With our dictionary and the dfm, we can then create a new dfm containing the counts of words found for each of the two sub-dictionaries. 
# We use our glob dictionary and specify in the `dfm_lookup` function to use the glob notation. 
# Note that in the case of sentiment analysis, the resulting dfm could e.g. have one column for positive and one column for negative sentiment keywords. 
# In our example it has one for taxation and one for unemployment words.


prince_dfm %>% dfm_select(pattern = "control*", valuetype = "glob") %>% featnames()
prince_dfm %>% dfm_select(pattern = "sovereign*", valuetype = "glob") %>% featnames()

prince_dfm %>% dfm_select(pattern = "Sovereign*", valuetype = "glob") %>% featnames()

prince_dfm_dictionary <- dfm_lookup(prince_dfm, dictionary = power_dictionary,
                             valuetype = "glob")
prince_dfm_dictionary


# With this we have the keyword counts for each document. 
# Yet, as documents can have different lengths, we might be interested in dividing the keyword counts by the total words contained in the documents.


prince_dfm_dictionary_relative <- prince_dfm_dictionary/rowSums(prince_dfm)
prince_dfm_dictionary_relative


## Wordcloud:

prince_dfm_cleaned <- prince_corp %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  dfm()

prince_dfm_cleaned




textplot_wordcloud(prince_dfm_cleaned,
                   rotation=0, min_size=.75, max_size=3, max_words=50)


###################################
#### Loading Hobbes.     ##########
###################################


leviathan <- read_file("~/Desktop/leviathan.txt")


leviathan_corpus <- corpus(leviathan, docvars = data.frame(name= "hobbes"))

leviathan_corpus_sent <- corpus_reshape(leviathan_corpus, to = "sentences")

# subsetting it: 
print(leviathan_corpus_sent[1:86])
leviathan_corpus_subset <- corpus_subset(leviathan_corpus_sent[86:6020])
leviathan_corp <- corpus_reshape(leviathan_corpus_subset, to = "documents")


##### creating dfm

leviathan_dfm <- leviathan_corp %>%
  tokens(remove_punct = TRUE, remove_symbols = TRUE) %>% 
  dfm()

leviathan_dfm 

#######

leviathan_dfm %>% dfm_select(pattern = "citizen*", valuetype = "glob") %>% featnames()

dfm

leviathan_dfm_dictionary <- dfm_lookup(leviathan_dfm, dictionary = power_dictionary,
                                    valuetype = "glob")
leviathan_dfm_dictionary


leviathan_dfm_dictionary_relative <- leviathan_dfm_dictionary/rowSums(leviathan_dfm)
leviathan_dfm_dictionary_relative

#### 3. Wordclouds

# Another frequently used approach to visualise outcomes in textual analysis are word clouds. 
# The size of words in the clouds depicts their relative frequency in the selected documents. 
# Before we depict such word clouds, let us additionally remove stop words and numbers from the corpus when creating a new dfm:



custom_stopwords <- c("made", "therefore", "yet", "upon", "though")

leviathan_dfm_cleaned <- leviathan_corp %>%
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_remove(stopwords("en")) %>%
  dfm()
leviathan_dfm_cleaned


# To depict word clouds for different presidents, we need to select associated documents/rows in the dfm. 
# This can e.g. be done via information contained in the document variables using the function `dfm_subset` which returns another dfm that is a subset of the original one. 
# We will then use this dfm as the  argument in the `textplot_wordcloud` function:
  

textplot_wordcloud(leviathan_dfm_cleaned,
                   rotation=0, min_size=.75, max_size=3, max_words=50)

