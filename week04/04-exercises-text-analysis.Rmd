---
title: "MY472 - Week 4: Seminar exercises in text analysis"
author: "Ryan Hübert"
date: "AT 2024"
output: html_document
---

**Attribution statement:** _The following teaching materials have been iteratively developed by current and former instructors, including: Friedrich Geiecke, Thomas Robinson and Ryan Hübert._



Suppose that you are doing a project analysing news articles from around the world in different languages. Your collaborator sends you a news article saved in a plain-text file. Unfortunately, he does not know how it is encoded, or even which languages it is in. Figure out the file's character encoding and its language (Google or ChatGPT can help you identify the language if you cannot recognize it yourself). Then write a new copy on your computer with UTF-8 encoding and using a file name that indicates it is UTF-8.

```{r}
library(tidyverse)
my.file <- "data/news-article-1.txt"
raw <- read_file(my.file, locale = locale(encoding="___________"))
print(raw) # this is __________ [language]
my.utf.file <- "___________"
write_file(___________, ___________)
```


## Exercise 1: character encoding


1.1 The file uol.txt in the course repo for this week contains a list of 17 member institutions of the University of London. In your text editor or in R via `str_view_all()`, write down a regular expression which finds all postcodes. Note that in some of the postcodes there are white spaces between the two parts and in others not. The regular expression has to work with both.


```{r}
dir <- "~/Desktop/Political Science/LSE/LSE Courses/LSE MRes Political Science/lse-my472.github.io/week04/data/"

uni_london <- read_file(paste0(dir,"uol.txt"))

str_view(uni_london, "[A-Z]{1,2}\\d{1,2}[A-Z]{0,1}\\s?\\d{0,1}[A-Z]{1,2}")
                        
                        
```



__Greedy vs. Ungreedy Quantifiers__

Greedy Quantifiers:

*: Matches 0 or more times, as many as possible.
+: Matches 1 or more times, as many as possible.
?: Matches 0 or 1 time, as many as possible.
{n,m}: Matches between n and m times, as many as possible.

Ungreedy (Lazy) Quantifiers:

*?: Matches 0 or more times, as few as possible.
+?: Matches 1 or more times, as few as possible.
??: Matches 0 or 1 time, as few as possible.
{n,m}?: Matches between n and m times, as few as possible.


```{r}
# Greedy Quantifier 

text <- "This is a <b>bold</b> statement."
greedy_match <- str_extract(text, "<.*>")
print(greedy_match)  # Output: "<b>bold</b> statement."
```


```{r}
# Ungreedy Quantifier

text <- "This is a <b>bold</b> statement."
ungreedy_match <- str_extract(text, "<.*?>")
print(ungreedy_match)  # Output: "<b>"

```


1.2 Next, try to mute/delete the second part of each postcode. Add a capturing group to your regular expression with which you can select only the first part, i.e. the first 2-4 characters of the postcodes. Then use find & replace in your text editor or `str_replace_all()` in R, and replace all postcodes with only the information stored in capturing group 1. This deletes the second part of each postcode.

Hint: In R, the way to reference the group in the replace is `"\\1"` instead of `$1` in the text editor.

__Capturing Group:__

A capturing group is a part of a regular expression that allows you to isolate and extract a specific portion of the matched text. Capturing groups are created by placing the desired pattern inside parentheses ()


`str_replace_all()`


```{r}
postcode <- str_replace_all(uni_london, "([A-Z]{1,2}\\d{1,2}[A-Z]{0,1})\\s?\\d{0,1}[A-Z]{1,2}", "\\1")
postcode

?str_replace_all()
```


## Exercise 2

Imagine you have a specific document and would like to find those documents in a large set/database of documents that are most similar to it. This first seems like a daunting task, but could be useful both in academic research and private sector work (imagine a law firm that is looking for similar cases or researchers looking for similar articles). How could a computer programme achieve something like this? The trick of one possible approach is to combine your knowledge about text analysis with a bit of geometry and linear algebra. First, realise that every row in a dfm is actually a (transposed) vector of length/dimension K where K is the amount of features in the dfm. For a very brief introduction to vectors, see e.g. this excellent [video](https://youtu.be/fNk_zzaMoSs).

Let us assume for a moment that we only have three features/words in a dfm. Then every row/document is a 3 dimensional vector of counts and we can think of each document like a point in 3 dimensional space such as the room in which you are sitting. Axis 1 would denote the count of word 1 in the documents, axis 2 the count of word 2, axis 3 the count of word 3. Different vectors/documents would be in different parts of the space depending on which words they contain. (Normalised) vectors/documents of similar textual content should be in similar corners or areas of the room or space. With some help from mathematics we can in fact compute how similar or close these vectors or points in space are also quantitatively. The most frequently used approach to compute similarities between numerical vectors of word counts, also in high dimensional spaces with many different words, is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).


2.1 First, create a dfm using the `data_char_ukimmig2010` object from `quanteda` which contains extracts from the election manifestos of 9 UK political parties from 2010, related to immigration or asylum-seekers. Transform it into a corpus, remove punctuation and stopwords, and stem words (`tokens_wordstem()`). Also remove all words which are not at least contained in 2 documents (this often makes similarities work better because the vectors contain fewer entries/dimensions and less noise).

```{r}
library("quanteda")
library("quanteda.textstats")
library("readtext")

?readtext()

```


`readtext()`


__Transposition of Vector__

A transposed vector is a vector that has been converted from a row vector to a column vector, or vice versa. In linear algebra, the transpose of a matrix is an operation that flips the matrix over its diagonal, switching the row and column indices of each element.

__Tokenization__

- Token =  a token is a single unit of text that is considered as a separate element for analysis.
- Tokenization is the process of breaking down a text into these individual units, which can be words, phrases, symbols, or other meaningful elements.

__Corpus__

A corpus (plural: corpora) is a large and structured set of texts. It is used for statistical analysis, hypothesis testing, and linguistic research.

```{r}
dfm <- data_char_ukimmig2010 %>%                        # 1. study the data
  corpus() %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>%    # 2. remove punctuation and numbers
  tokens_remove(stopwords("en")) %>%                        # 3. remove stopwords (English)
  tokens_wordstem() %>%                                     # 4. stem the tokens
  dfm() %>%
  dfm_trim(min_termfreq = 2)                                 # 5. minimum word frequency = 2


dfm

print(dfm)


```

__Accessing the dfm__

```{r}
docnames(dfm)
featnames(dfm)


dfm[,1] # accessing features 
dfm[, "immigration"]


dfm[1,] # accessing documents 
dfm["BNP",]



```



```{r}
# test test str_sub()


string_test <- "This is a veryveryVERY long sentence."

str_sub(string_test, 1,7)

str_sub(string_test, 1, 7) <- "this isn't"
```




## References

- https://quanteda.io/articles/quickstart.html